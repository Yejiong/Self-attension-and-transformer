{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb4a5057",
   "metadata": {},
   "source": [
    "# Task description\n",
    "- Classify the speakers of given features.\n",
    "- Baselines:\n",
    "  - Easy: Run sample code and know how to use transformer.\n",
    "  - Medium: Know how to adjust parameters of transformer.\n",
    "  - Strong: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer. \n",
    "  - Boss: Implement [Self-Attention Pooling](https://arxiv.org/pdf/2008.01077v1.pdf) & [Additive Margin Softmax](https://arxiv.org/pdf/1801.05599.pdf) to further boost the performance.\n",
    "\n",
    "\n",
    "# Download dataset\n",
    "- Data is [here](https://drive.google.com/drive/folders/1vI1kuLB-q1VilIftiwnPOCAeOOFfBZge?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5907d4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anacoda\\envs\\Pytorch_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import module\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# seed setting\n",
    "def same_seeds(seed):\n",
    "    # Python built-in random module\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(1212)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19a4e4",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Dataset\n",
    "- Original dataset is [Voxceleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html).\n",
    "- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb2.\n",
    "- We randomly select 600 speakers from Voxceleb2.\n",
    "- Then preprocess the raw waveforms into mel-spectrograms.\n",
    "\n",
    "- Args:\n",
    "  - data_dir: The path to the data directory.\n",
    "  - metadata_path: The path to the metadata.\n",
    "  - segment_len: The length of audio segment for training. \n",
    "- The architecture of data directory \\\\\n",
    "  - data directory \\\\\n",
    "  |---- metadata.json \\\\\n",
    "  |---- testdata.json \\\\\n",
    "  |---- mapping.json \\\\\n",
    "  |---- uttr-{random string}.pt \\\\\n",
    "\n",
    "- The information in metadata\n",
    "  - \"n_mels\": The dimention of mel-spectrogram.\n",
    "  - \"speakers\": A dictionary. \n",
    "    - Key: speaker ids.\n",
    "    - value: \"feature_path\" and \"mel_len\"\n",
    "\n",
    "\n",
    "For efficiency, we segment the mel-spectrograms into segments in the traing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c82cc67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "## Create my dataset\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, data_dir, segment_length=128):\n",
    "        self.data_dir = data_dir\n",
    "        self.segment_length = segment_length\n",
    "    \n",
    "        # Load the mapping from speaker name to their corresponding id. \n",
    "        mapping_path = os.path.join(data_dir, 'mapping.json')\n",
    "        self.speaker2id = json.load(open(mapping_path))['speaker2id']\n",
    "        \n",
    "        # Load metadata of training data.\n",
    "        metadata_path = os.path.join(data_dir, 'metadata.json')\n",
    "        metadata = json.load(open(metadata_path))['speakers']\n",
    "        \n",
    "        # Get the total number of speaker\n",
    "        self.speaker_num = len(metadata)\n",
    "        self.data = []\n",
    "        for speaker in metadata.keys():\n",
    "            for voice in metadata[speaker]:\n",
    "                self.data.append([voice['feature_path'], self.speaker2id[speaker]])\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fpath, speaker = self.data[idx]\n",
    "        # Load preprocessed mel-spectrogram.\n",
    "        mel = torch.load(os.path.join(self.data_dir, fpath))\n",
    "        \n",
    "        # Segmemt mel-spectrogram into \"segment_len\" frames.\n",
    "        if len(mel) > self.segment_length:\n",
    "            # Randomly get the starting point of the segment.\n",
    "            start = random.randint(0, len(mel) - self.segment_length)\n",
    "            # Get a segment with \"segment_len\" frames.\n",
    "            mel = torch.Tensor(mel[start:(start+self.segment_length)])\n",
    "        else:\n",
    "            mel = torch.Tensor(mel)\n",
    "        # Turn the speaker id into long for computing loss later.\n",
    "        speaker = torch.tensor(speaker)\n",
    "        return mel, speaker\n",
    "    \n",
    "    def get_speaker_num(self):\n",
    "        return self.speaker_num\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d88f7",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "- Split dataset into training dataset(90%) and validation dataset(10%).\n",
    "- Create dataloader to iterate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a74bde8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    # Process features within a batch.\n",
    "    \"\"\"Collate a batch of data.\"\"\"\n",
    "    mel, speaker = list(zip(*batch))\n",
    "    # Because we train the model batch by batch, we need to pad \n",
    "    # the features in the same batch to make their lengths the same.\n",
    "    \n",
    "    mel = pad_sequence(mel, batch_first = True, padding_value = -20)\n",
    "    return mel, torch.Tensor(speaker).long()\n",
    "\n",
    "\n",
    "def get_dataloader(data_dir, batch_size):\n",
    "    \"\"\"\n",
    "    Generate dataloader\n",
    "    \"\"\"\n",
    "    dataset = Mydataset(data_dir)\n",
    "    speaker_num = dataset.get_speaker_num()\n",
    "    # Split dataset into training dataset and validation dataset\n",
    "    train_size = int(0.9*len(dataset))\n",
    "    valid_size = len(dataset) - train_size\n",
    "    trainset, validset = random_split(dataset, lengths=[train_size, valid_size])\n",
    "    \n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True,\n",
    "                              drop_last=True, pin_memory=True, \n",
    "                              collate_fn=collate_batch)\n",
    "    \n",
    "    valid_loader = DataLoader(validset, batch_size=batch_size, shuffle=True,\n",
    "                              drop_last=True, pin_memory=True, \n",
    "                              collate_fn=collate_batch)\n",
    "    \n",
    "    return train_loader, valid_loader, speaker_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b6b75",
   "metadata": {},
   "source": [
    "# Model\n",
    "- TransformerEncoderLayer:\n",
    "  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "  - Parameters:\n",
    "    - d_model: the number of expected features of the input (required).\n",
    "\n",
    "    - nhead: the number of heads of the multiheadattention models (required).\n",
    "\n",
    "    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "\n",
    "    - dropout: the dropout value (default=0.1).\n",
    "\n",
    "    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "- TransformerEncoder:\n",
    "  - TransformerEncoder is a stack of N transformer encoder layers\n",
    "  - Parameters:\n",
    "    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
    "\n",
    "    - num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "\n",
    "    - norm: the layer normalization component (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff64a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline model\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.prenet = nn.Linear(40, d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, batch_first=True, \n",
    "                                                        dim_feedforward=256, nhead=4, \n",
    "                                                        norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
    "        \n",
    "        self.pred_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, n_spks)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, length, 40)\n",
    "        out: (batch, size, n_spks)\n",
    "        \"\"\"\n",
    "        y = self.prenet(x)\n",
    "        y = self.encoder(y)\n",
    "        # mean pooling\n",
    "        y = y.mean(dim = 1)\n",
    "        y = self.pred_layer(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77851025",
   "metadata": {},
   "source": [
    "### Build the [conformer](https://arxiv.org/abs/2005.08100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38d2966f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Feedforward Module\n",
    "class Feedforwardmodule(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_p=0.2, expansion_factor=2):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, embedding_dim * expansion_factor),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(embedding_dim * expansion_factor, embedding_dim),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.layer(x)\n",
    "        return x + y\n",
    "\n",
    "## Multi-head self-attention module\n",
    "class MHST(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dropout_p=0.2):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        self.Mulithead = nn.MultiheadAttention(embedding_dim, num_heads=num_heads,\n",
    "                                               batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        self.Q_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.K_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.V_layer = nn.Linear(embedding_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.layer_norm(inputs)\n",
    "        Q = self.Q_layer(x)\n",
    "        K = self.K_layer(x)\n",
    "        V = self.V_layer(x)\n",
    "        y, x_weight = self.Mulithead(Q,K,V)\n",
    "        y = self.dropout(y)\n",
    "        \n",
    "        return inputs + y\n",
    "\n",
    "    \n",
    "## Convolution module\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, dim0, dim1):\n",
    "        super().__init__()\n",
    "        self.dim0 = dim0\n",
    "        self.dim1 = dim1\n",
    "    def forward(self, x):\n",
    "        return x.transpose(self.dim0, self.dim1)\n",
    "\n",
    "class ConvolutionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    input shape: (B, L, in_channels)\n",
    "    \"\"\"  \n",
    "    def __init__(self, in_channels, kernal_size=3, dropout_p=0.2):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.LayerNorm(in_channels), ## shape: (B, L, in_channels)\n",
    "            Transpose(1,2), ## shape: (B, in_channels, L),\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=in_channels*2,\n",
    "                      kernel_size=1), ## shape: (B, 2*in_channels, L)\n",
    "            nn.GLU(dim=1), ## shape: (B, in_channels, L)\n",
    "            nn.Conv1d(in_channels=in_channels, out_channels=in_channels,\n",
    "                      kernel_size=kernal_size, groups=in_channels,padding=1),## shape : (B, out_channels, L)\n",
    "            nn.BatchNorm1d(in_channels), ## shape: (B, out_channels, L)\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(in_channels=in_channels,out_channels=in_channels, kernel_size=1),\n",
    "            nn.Dropout(p=dropout_p)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.layer(x)\n",
    "        y = y.transpose(1,2)\n",
    "        return x+y\n",
    "    \n",
    "        \n",
    "\n",
    "## Conformer layer    \n",
    "class Conformer(nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.FFL1 = Feedforwardmodule(embedding_dim=embedding_dim)\n",
    "        self.mhst = MHST(embedding_dim=embedding_dim, num_heads = 5)\n",
    "        self.convolution = ConvolutionModule(in_channels = embedding_dim)\n",
    "        self.FFL2 = Feedforwardmodule(embedding_dim=embedding_dim)\n",
    "        self.layernorm = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y1 = 0.5*self.FFL1(x) + x\n",
    "        y2 = self.mhst(y1) +y1\n",
    "        y3 = self.convolution(y2) + y2\n",
    "        y4 = 0.5*self.FFL2(y3) + y3\n",
    "        output = self.layernorm(y4)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "## conformer encode model\n",
    "\n",
    "class ConformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conformers = nn.ModuleList([Conformer(embedding_dim=80) for i in range(6)])\n",
    "        self.prenet = nn.Sequential(\n",
    "            nn.Linear(40, d_model),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "        self.pred_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, n_spks)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.prenet(x)\n",
    "        \n",
    "        for layer in self.conformers:\n",
    "            x1 = layer(x1)\n",
    "        \n",
    "        x2 = x1.mean(dim=1)\n",
    "        output = self.pred_layer(x2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "843d8815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 600])\n",
      "764920\n"
     ]
    }
   ],
   "source": [
    "conformer_model = ConformerEncoder()\n",
    "x = torch.randn(32, 108, 40)\n",
    "y = conformer_model(x)\n",
    "print(y.shape)\n",
    "\n",
    "num_pars = 0\n",
    "for par in conformer_model.parameters():\n",
    "    num_pars += par.numel()\n",
    "print(num_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c147dd5",
   "metadata": {},
   "source": [
    "## Learning rate schedule\n",
    "- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n",
    "-The warmup of learning rate is useful for training models with transformer architectures.\n",
    "- The warmup schedule\n",
    "  - Set learning rate to 0 in the beginning.\n",
    "  - The learning rate increases linearly from 0 to initial learning rate during warmup period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d83f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "def lr_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps,\n",
    "                            num_cycles=0.5, last_epoch=-1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases following the values \n",
    "    of the cosine function between the initial lr set in the optimizer to 0, \n",
    "    after a warmup period during which it increases linearly between 0 and the\n",
    "    initial lr set in the optimizer.\n",
    "\n",
    "    Args:\n",
    "        optimizer (:class:`~torch.optim.Optimizer`):\n",
    "        The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (:obj:`int`):\n",
    "        The number of steps for the warmup phase.\n",
    "        num_training_steps (:obj:`int`):\n",
    "        The total number of training steps.\n",
    "        num_cycles (:obj:`float`, `optional`, defaults to 0.5):\n",
    "        The number of waves in the cosine schedule \n",
    "        (the defaults is to just decrease from the max value to 0\n",
    "        following a half-cosine).\n",
    "        last_epoch (:obj:`int`, `optional`, defaults to -1):\n",
    "        The index of the last epoch when resuming training.\n",
    "\n",
    "    Return:\n",
    "        :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"  \n",
    "\n",
    "    def lr_lambda(current_step):\n",
    "        # warm up\n",
    "        if current_step <= num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        \n",
    "        # after warm up\n",
    "        progress = float(current_step - num_warmup_steps) / float(\n",
    "            max(1, num_training_steps - num_warmup_steps)\n",
    "        )\n",
    "        return max(\n",
    "            0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n",
    "        )\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de344f8",
   "metadata": {},
   "source": [
    "## Train&Validate process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78c1e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model forward function\n",
    "\n",
    "def model_fn(batch, model, criterion, device):\n",
    "    model.train()\n",
    "    mels, labels = batch\n",
    "    mels = mels.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    out = model(mels)\n",
    "    \n",
    "    loss = criterion(out, labels)\n",
    "    \n",
    "    pred = out.argmax(dim = 1)\n",
    "    accuracy = torch.mean((pred == labels).float())\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def valid(dataloader, model, criterion, device):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    loss_seq = []\n",
    "    accuracy_seq = []\n",
    "    bar = tqdm(dataloader, desc = 'Valid')\n",
    "    \n",
    "    for mels, labels in bar:\n",
    "        mels = mels.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(mels)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        preds = outputs.argmax(dim = 1)\n",
    "        \n",
    "        accuracy = torch.mean((preds == labels).float())\n",
    "        bar.set_postfix(loss = loss.item(), accuracy = accuracy.item())\n",
    "        loss_seq.append(loss.item())\n",
    "        accuracy_seq.append(accuracy.item())\n",
    "    bar.close()    \n",
    "    model.train()\n",
    "    return np.mean(loss_seq), np.mean(accuracy_seq)\n",
    "    \n",
    "\n",
    "\n",
    "def train(config):\n",
    "    \n",
    "    data_dir = config['data_dir']\n",
    "    save_path = config['save_path']\n",
    "    batch_size = config['batch_size']\n",
    "    warmup_steps = config['warmup_steps']\n",
    "    valid_steps = config['valid_steps']\n",
    "    train_steps = config['train_steps']\n",
    "    early_stops = config['early_stops']\n",
    "    model_type = config['model_type']\n",
    "    \n",
    "    # set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'[Info]: {device} is used')\n",
    "    \n",
    "    # load data\n",
    "    trainloader, validloader, speaker_num = get_dataloader(data_dir, batch_size)\n",
    "    \n",
    "    # creating model\n",
    "    if model_type == 'classic':\n",
    "        model = Classifier(n_spks = speaker_num).to(device)\n",
    "    elif model_type == 'conformer':\n",
    "        model = ConformerEncoder(n_spks=speaker_num).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    lr_scheduler = lr_schedule_with_warmup(optimizer,warmup_steps, train_steps)\n",
    "    \n",
    "    best_accuracy = -1\n",
    "    early_step = 0\n",
    "    \n",
    "    for e in range(train_steps):\n",
    "        \n",
    "        train_bar = tqdm(trainloader)\n",
    "        train_bar.set_description(f'Epoch: {e}/{train_steps}')\n",
    "        for batch in train_bar:\n",
    "            loss, accuracy = model_fn(batch, model, criterion, device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            train_bar.set_postfix(loss = loss.item(), accuracy = accuracy.item())\n",
    "            \n",
    "    \n",
    "        if (e + 1)%valid_steps == 0:\n",
    "            valid_loss, valid_accuracy = valid(validloader, model, criterion, device)\n",
    "        \n",
    "            if valid_accuracy > best_accuracy:\n",
    "                best_accuracy = valid_accuracy\n",
    "                best_state_dict = model.state_dict()\n",
    "                torch.save(best_state_dict, os.path.join(save_path, f'{model_type}_{e}.pth'))\n",
    "                print('The best model is saved')\n",
    "                early_step = 0\n",
    "            else:\n",
    "                early_step += 1\n",
    "    \n",
    "        if early_step > early_stops:\n",
    "            print(\"The model didn't improve, so we decide to shut down it\")\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a60aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data_dir\": \"D://Datasets/Dataset/Dataset/\",\n",
    "    \"save_path\": \"D:\\Dropbox\\Deep learning\\Deep Learning Hongyi Li\\\\4\\save_models\",\n",
    "    \"batch_size\": 32,\n",
    "    \"valid_steps\": 2,\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"save_steps\": 10000,\n",
    "    \"train_steps\": 70000,\n",
    "    'early_stops':5,\n",
    "    'model_type':'conformer'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e90c25b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info]: cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/70000: 100%|██████████████████████████████████| 1593/1593 [06:05<00:00,  4.36it/s, accuracy=0.0938, loss=4.35]\n",
      "Epoch: 1/70000: 100%|████████████████████████████████████| 1593/1593 [01:28<00:00, 17.92it/s, accuracy=0.25, loss=3.47]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:40<00:00,  4.40it/s, accuracy=0.281, loss=3.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2/70000: 100%|███████████████████████████████████| 1593/1593 [01:28<00:00, 17.91it/s, accuracy=0.312, loss=2.76]\n",
      "Epoch: 3/70000: 100%|███████████████████████████████████| 1593/1593 [01:28<00:00, 17.98it/s, accuracy=0.531, loss=2.51]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:03<00:00, 45.51it/s, accuracy=0.531, loss=2.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4/70000: 100%|███████████████████████████████████| 1593/1593 [01:28<00:00, 18.01it/s, accuracy=0.531, loss=2.11]\n",
      "Epoch: 5/70000: 100%|███████████████████████████████████| 1593/1593 [01:28<00:00, 17.97it/s, accuracy=0.406, loss=2.54]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:03<00:00, 46.05it/s, accuracy=0.219, loss=3.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6/70000: 100%|███████████████████████████████████| 1593/1593 [01:28<00:00, 17.92it/s, accuracy=0.531, loss=1.85]\n",
      "Epoch: 7/70000: 100%|████████████████████████████████████| 1593/1593 [01:29<00:00, 17.85it/s, accuracy=0.531, loss=2.2]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:03<00:00, 45.18it/s, accuracy=0.625, loss=1.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8/70000: 100%|███████████████████████████████████| 1593/1593 [01:29<00:00, 17.88it/s, accuracy=0.438, loss=1.81]\n",
      "Epoch: 9/70000: 100%|███████████████████████████████████| 1593/1593 [01:28<00:00, 17.96it/s, accuracy=0.531, loss=1.97]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:03<00:00, 45.99it/s, accuracy=0.469, loss=2.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10/70000: 100%|██████████████████████████████████| 1593/1593 [01:29<00:00, 17.82it/s, accuracy=0.562, loss=1.85]\n",
      "Epoch: 11/70000: 100%|██████████████████████████████████| 1593/1593 [01:29<00:00, 17.89it/s, accuracy=0.625, loss=1.55]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:03<00:00, 46.14it/s, accuracy=0.625, loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 12/70000: 100%|██████████████████████████████████| 1593/1593 [01:29<00:00, 17.89it/s, accuracy=0.594, loss=1.53]\n",
      "Epoch: 13/70000: 100%|██████████████████████████████████| 1593/1593 [01:28<00:00, 18.06it/s, accuracy=0.594, loss=1.85]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:03<00:00, 46.48it/s, accuracy=0.719, loss=1.47]\n",
      "Epoch: 14/70000: 100%|██████████████████████████████████| 1593/1593 [01:28<00:00, 17.94it/s, accuracy=0.562, loss=1.69]\n",
      "Epoch: 15/70000: 100%|██████████████████████████████████| 1593/1593 [01:30<00:00, 17.67it/s, accuracy=0.594, loss=1.21]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:03<00:00, 44.42it/s, accuracy=0.625, loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 16/70000: 100%|██████████████████████████████████| 1593/1593 [01:29<00:00, 17.77it/s, accuracy=0.625, loss=1.53]\n",
      "Epoch: 17/70000: 100%|██████████████████████████████████| 1593/1593 [01:29<00:00, 17.85it/s, accuracy=0.688, loss=1.06]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:03<00:00, 45.45it/s, accuracy=0.688, loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 18/70000: 100%|██████████████████████████████████| 1593/1593 [01:29<00:00, 17.83it/s, accuracy=0.75, loss=0.936]\n",
      "Epoch: 19/70000: 100%|████████████████████████████████████| 1593/1593 [01:29<00:00, 17.87it/s, accuracy=0.5, loss=2.05]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:03<00:00, 45.47it/s, accuracy=0.594, loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 20/70000: 100%|██████████████████████████████████| 1593/1593 [01:29<00:00, 17.87it/s, accuracy=0.75, loss=0.893]\n",
      "Epoch: 21/70000: 100%|██████████████████████████████████| 1593/1593 [01:29<00:00, 17.85it/s, accuracy=0.781, loss=1.04]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:03<00:00, 45.23it/s, accuracy=0.75, loss=0.911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 22/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.76it/s, accuracy=0.844, loss=0.757]\n",
      "Epoch: 23/70000: 100%|██████████████████████████████████| 1593/1593 [01:29<00:00, 17.78it/s, accuracy=0.812, loss=0.52]\n",
      "Valid: 100%|███████████████████████████████████████████████| 177/177 [00:03<00:00, 45.85it/s, accuracy=0.812, loss=1.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 24/70000: 100%|██████████████████████████████████| 1593/1593 [01:29<00:00, 17.79it/s, accuracy=0.75, loss=0.885]\n",
      "Epoch: 25/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.79it/s, accuracy=0.812, loss=0.568]\n",
      "Valid: 100%|███████████████████████████████████████████████| 177/177 [00:03<00:00, 45.45it/s, accuracy=0.75, loss=1.39]\n",
      "Epoch: 26/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.74it/s, accuracy=0.875, loss=0.687]\n",
      "Epoch: 27/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.87it/s, accuracy=0.812, loss=0.762]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:03<00:00, 45.52it/s, accuracy=0.656, loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 28/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.82it/s, accuracy=0.844, loss=0.717]\n",
      "Epoch: 29/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.80it/s, accuracy=0.906, loss=0.477]\n",
      "Valid: 100%|█████████████████████████████████████████████| 177/177 [00:03<00:00, 44.76it/s, accuracy=0.812, loss=0.758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 30/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.81it/s, accuracy=0.844, loss=0.602]\n",
      "Epoch: 31/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.82it/s, accuracy=0.812, loss=0.593]\n",
      "Valid: 100%|█████████████████████████████████████████████| 177/177 [00:03<00:00, 45.35it/s, accuracy=0.719, loss=0.803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 32/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.71it/s, accuracy=0.781, loss=0.576]\n",
      "Epoch: 33/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.74it/s, accuracy=0.719, loss=0.877]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:03<00:00, 45.42it/s, accuracy=0.719, loss=1.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 34/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.75it/s, accuracy=0.844, loss=0.582]\n",
      "Epoch: 35/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.75it/s, accuracy=0.812, loss=0.416]\n",
      "Valid: 100%|███████████████████████████████████████████████| 177/177 [00:03<00:00, 45.73it/s, accuracy=0.75, loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 36/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.82it/s, accuracy=0.812, loss=0.666]\n",
      "Epoch: 37/70000: 100%|█████████████████████████████████| 1593/1593 [01:29<00:00, 17.76it/s, accuracy=0.875, loss=0.536]\n",
      "Valid: 100%|██████████████████████████████████████████████| 177/177 [00:03<00:00, 45.67it/s, accuracy=0.844, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 38/70000: 100%|██████████████████████████████████| 1593/1593 [01:29<00:00, 17.75it/s, accuracy=0.938, loss=0.33]\n",
      "Epoch: 39/70000: 100%|█████████████████████████████████| 1593/1593 [01:30<00:00, 17.66it/s, accuracy=0.906, loss=0.401]\n",
      "Valid: 100%|█████████████████████████████████████████████| 177/177 [00:03<00:00, 44.31it/s, accuracy=0.812, loss=0.391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 40/70000:   6%|██▏                               | 102/1593 [00:05<01:25, 17.36it/s, accuracy=0.875, loss=0.296]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     78\u001b[0m train_bar \u001b[38;5;241m=\u001b[39m tqdm(trainloader)\n\u001b[0;32m     79\u001b[0m train_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_bar:\n\u001b[0;32m     81\u001b[0m     loss, accuracy \u001b[38;5;241m=\u001b[39m model_fn(batch, model, criterion, device)\n\u001b[0;32m     83\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mD:\\Anacoda\\envs\\Pytorch_env\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anacoda\\envs\\Pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\Anacoda\\envs\\Pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\Anacoda\\envs\\Pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Anacoda\\envs\\Pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Anacoda\\envs\\Pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mMydataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     28\u001b[0m fpath, speaker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Load preprocessed mel-spectrogram.\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m mel \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Segmemt mel-spectrogram into \"segment_len\" frames.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mel) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegment_length:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Randomly get the starting point of the segment.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anacoda\\envs\\Pytorch_env\\lib\\site-packages\\torch\\serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mD:\\Anacoda\\envs\\Pytorch_env\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mD:\\Anacoda\\envs\\Pytorch_env\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c91de",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a6e7e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "## Inference dataset\n",
    "class Inferencedataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        testdata_path = os.path.join(data_dir, 'testdata.json')\n",
    "        self.data = json.load(open(testdata_path))['utterances']\n",
    "        self.data_dir = data_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        utterance = self.data[idx]\n",
    "        feat_path = utterance['feature_path']\n",
    "        mel = torch.load(os.path.join(self.data_dir, feat_path))\n",
    "        return feat_path, mel\n",
    "\n",
    "def inference_collate_bath(batch):\n",
    "    feat_pathes, mel = list(zip(*batch))\n",
    "    return feat_pathes, torch.stack(mel)\n",
    "\n",
    "def Inference(config):\n",
    "    \n",
    "    data_dir = config['data_dir']\n",
    "    model_path = config['model_path']\n",
    "    output_path = config['output_path']\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    mapping = json.load(open(os.path.join(data_dir, 'mapping.json')))\n",
    "    \n",
    "    dataset = Inferencedataset(data_dir)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, drop_last=False,\n",
    "                            collate_fn=inference_collate_bath)\n",
    "    \n",
    "    speaker_num = len(mapping['id2speaker'])\n",
    "    \n",
    "    model = Classifier(n_spks=speaker_num).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    results = [['Id', 'Category']]\n",
    "    for feat_path, mel in tqdm(dataloader):\n",
    "        with torch.no_grad():\n",
    "            mel = mel.to(device)\n",
    "            out = model(mel)\n",
    "            pred = out.argmax(dim = 1).cpu().numpy()\n",
    "            for f, p in zip(feat_path, pred):\n",
    "                results.append([f, mapping[\"id2speaker\"][str(pred[0])]])\n",
    "        \n",
    "    with open(output_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5ad8535f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_inf = {\n",
    "    'data_dir': \"D://Datasets/Dataset/Dataset/\",\n",
    "    'model_path': 'D:\\Dropbox\\Deep learning\\Deep Learning Hongyi Li\\\\4\\save_models\\model_51.pth',\n",
    "    'output_path': 'D:\\Dropbox\\Deep learning\\Deep Learning Hongyi Li\\\\4\\prediction.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "be99fb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:14<00:00, 559.24it/s]\n"
     ]
    }
   ],
   "source": [
    "Inference(config_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdf3b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Mydataset(config['data_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ba9ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 108, 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
